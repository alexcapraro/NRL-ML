{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module to set up Chrome Web Driver for Scalping.\n",
    "\n",
    "This module provides a function to set up the Chrome Web Driver with specific options\n",
    "for automated web scraping tasks related to Scalping.\n",
    "\n",
    "This script was taken from https://github.com/beauhobba/NRL-Data/.\n",
    "\n",
    "Future work may be needed to make this more efficient as it takes ~8 minutes to run each year.\n",
    "\"\"\"\n",
    "\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "def set_up_driver():\n",
    "    \"\"\"Set up the Chrome Web Driver for Scraping.\n",
    "\n",
    "    This function sets up the Chrome Web Driver with specified options.\n",
    "    \n",
    "    :return: WebDriver object for Chrome\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    # Ignore annoying messages from the NRL website \n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    \n",
    "    # Run Selenium in headless mode\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('log-level=3')\n",
    "    \n",
    "    # Exclude logging to assist with errors caused by NRL website \n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Webscraper for finding NRL data related to team statistics\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "#from utilities.set_up_driver import set_up_driver\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('..')\n",
    "#import ENVIRONMENT_VARIABLES as EV\n",
    "\n",
    "def get_nrl_data(round=1, year=1):\n",
    "    url = f\"https://www.nrl.com/draw/?competition=111&round={round}&season={year}\"\n",
    "    # Webscrape the NRL WEBSITE\n",
    "    driver = set_up_driver() \n",
    "    driver.get(url)\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # get the goodies\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    # Get the NRL data box\n",
    "    match_elements = soup.find_all(\n",
    "        \"div\", class_=\"match o-rounded-box o-shadowed-box\")\n",
    "\n",
    "    # name of html elements to poach from the data to get the nrl specific attributes\n",
    "    find_data = [\"h3\", \"p\", \"p\", \"div\", \"p\", \"div\", \"p\"]\n",
    "    class_data = [\"u-visually-hidden\", \"match-header__title\", \"match-team__name--home\",\n",
    "                  \"match-team__score--home\", \"match-team__name--away\", \"match-team__score--away\", \"match-venue o-text\"]\n",
    "\n",
    "    # Extract all the useful game data\n",
    "    matches_json = []\n",
    "    for match_element in match_elements:\n",
    "        match_details, match_date, home_team, home_score, away_team, away_score, venue = [match_element.find(\n",
    "            html_val, class_=class_val).text.strip() for html_val, class_val in zip(find_data, class_data)]\n",
    "\n",
    "        match = {\n",
    "            \"Details\": match_details.replace(\"Match: \", \"\"),\n",
    "            \"Date\": match_date,\n",
    "            \"Home\": home_team,\n",
    "            \"Home_Score\": home_score.replace(\"Scored\", \"\").replace(\"points\", \"\").strip(),\n",
    "            \"Away\": away_team,\n",
    "            \"Away_Score\": away_score.replace(\"Scored\", \"\").replace(\"points\", \"\").strip(),\n",
    "            \"Venue\": venue.replace(\"Venue:\", \"\").strip()\n",
    "        }\n",
    "        matches_json.append(match)\n",
    "    round_data = {\n",
    "        f\"{round}\": matches_json\n",
    "    }\n",
    "    return round_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#from nrl.scraping.utilities.get_nrl_data import get_nrl_data\n",
    "import json\n",
    "\n",
    "years = [2024]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    match_json_datas = []  # List to store JSON data for matches\n",
    "    for year in years:\n",
    "        year_json_data = []  # List to store JSON data for a particular year\n",
    "        for round_nu in range(1, 2):  # Loop through 31 rounds - In 2024, the Grand Final was round 31.\n",
    "            try:\n",
    "                # Attempt to fetch NRL data for a specific round and year\n",
    "                match_json = get_nrl_data(round_nu, year)\n",
    "                # Append fetched JSON to year's data list\n",
    "                year_json_data.append(match_json)\n",
    "            except Exception as ex:\n",
    "                print(f\"Error: {ex}\")\n",
    "        # Store year's data in a dictionary\n",
    "        year_data = {\n",
    "            f\"{year}\": year_json_data\n",
    "        }\n",
    "        # Append year's data to the main list\n",
    "        match_json_datas.append(year_data)\n",
    "\n",
    "    # Create overall data dictionary\n",
    "    overall_data = {\n",
    "        \"NRL\": match_json_datas\n",
    "    }\n",
    "    # Convert overall data to JSON format with indentation for better readability\n",
    "    overall_data_json = json.dumps(overall_data, indent=4)\n",
    "\n",
    "    # Write JSON data to a file\n",
    "    with open(\"D:\\\\Downloads\\\\match_data.json\", \"w\") as file:\n",
    "        file.write(overall_data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "# Convert JSON to table format\n",
    "\n",
    "with open('D:\\\\Downloads\\\\match_data_2024.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Function to remove special characters and new line characters from a string\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)  # Remove non-ASCII characters\n",
    "    text = text.replace('\\n', ' ').strip()  # Remove new line characters and strip leading/trailing spaces\n",
    "    text = re.sub(r'     .*', '', text) # Remove text after multiple spaces\n",
    "    text = re.sub(r'Home of the.*', '', text)\n",
    "    return text\n",
    "\n",
    "# Convert JSON to table format\n",
    "table_data = []\n",
    "headers = [\"Competition\", \"Year\", \"Round\", \"Details\", \"Date\", \"Home\", \"Home_Score\", \"Away\", \"Away_Score\", \"Venue\"]\n",
    "\n",
    "for competition, years in json_data.items():\n",
    "    for year_data in years:\n",
    "        for year, rounds in year_data.items():\n",
    "            for round_data in rounds:\n",
    "                for round_num, matches in round_data.items():\n",
    "                    for match in matches:\n",
    "                        # Clean the 'Venue' item\n",
    "                        match['Venue'] = clean_text(match['Venue'])\n",
    "                        row = [competition, year, round_num]\n",
    "                        row.extend([match[header] for header in headers[3:]])\n",
    "                        table_data.append(row)\n",
    "\n",
    "# Write table to txt file\n",
    "with open(\"D:\\\\Downloads\\\\nrl_table.txt\", \"w\") as file:\n",
    "    file.write(\"\\t\".join(headers) + \"\\n\")\n",
    "    for row in table_data:\n",
    "        file.write(\"\\t\".join(row) + \"\\n\")\n",
    "\n",
    "print(\"Table has been written to nrl_table.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from the 2024 season\n",
      "Scraping Round 1: 1 - https://www.nrl.com/draw/nrl-premiership/2024/round-1/sea-eagles-v-rabbitohs/\n",
      "Scraping Round 1: 1 - https://www.nrl.com/draw/nrl-premiership/2024/round-1/roosters-v-broncos/\n",
      "Tables has been written as D:\\Downloads\\2024_rd1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Webscraper for detailed NRL data related to team statistics\n",
    "\"\"\"\n",
    "\n",
    "import chromedriver_autoinstaller\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import sys  # noqa\n",
    "\n",
    "chromedriver_autoinstaller.install()\n",
    "\n",
    "def set_up_driver():\n",
    "    \"\"\"Set up the Chrome Web Driver for Scraping.\n",
    "\n",
    "    This function sets up the Chrome Web Driver with specified options.\n",
    "    \n",
    "    :return: WebDriver object for Chrome\n",
    "    \"\"\"\n",
    "    options = Options()\n",
    "    # Ignore annoying messages from the NRL website \n",
    "    options.add_argument('--ignore-certificate-errors')\n",
    "    \n",
    "    # Run Selenium in headless mode\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('log-level=3')\n",
    "    \n",
    "    # Exclude logging to assist with errors caused by NRL website \n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "\n",
    "#from utilities.set_up_driver import set_up_driver\n",
    "\n",
    "\n",
    "#sys.path.append('..')  # noqa\n",
    "#sys.path.append('..')  # noqa\n",
    "#import ENVIRONMENT_VARIABLES as EV  # noqa\n",
    "\n",
    "BARS_DATA: dict = {'time_in_possession': -1,\n",
    "                   'all_runs': -1,\n",
    "                   'all_run_metres': -1,\n",
    "                   'post_contact_metres': -1,\n",
    "                   'line_breaks': -1,\n",
    "                   'tackle_breaks': -1,\n",
    "                   'average_set_distance': -1,\n",
    "                   'kick_return_metres': -1,\n",
    "                   'offloads': -1,\n",
    "                   'receipts': -1,\n",
    "                   'total_passes': -1,\n",
    "                   'dummy_passes': -1,\n",
    "                   'kicks': -1,\n",
    "                   'kicking_metres': -1,\n",
    "                   'forced_drop_outs': -1,\n",
    "                   'bombs': -1,\n",
    "                   'grubbers': -1,\n",
    "                   'tackles_made': -1,\n",
    "                   'missed_tackles': -1,\n",
    "                   'intercepts': -1,\n",
    "                   'ineffective_tackles': -1,\n",
    "                   'errors': -1,\n",
    "                   'penalties_conceded': -1,\n",
    "                   'ruck_infringements': -1,\n",
    "                   'inside_10_metres': -1,\n",
    "                   'interchanges_used': -1}\n",
    "\n",
    "DONUT_DATA = {\n",
    "        'Completion Rate': -1,\n",
    "        'Average_Play_Ball_Speed': -1,\n",
    "        'Kick_Defusal': -1,\n",
    "        'Effective_Tackle': -1}\n",
    "\n",
    "def get_detailed_nrl_data(input_years, round_range, home_team, away_team):\n",
    "    \n",
    "    DONUT_DATA_2 = {'tries': -1, 'conversions': -1, 'penalty_goals':-1, 'sin_bins': -1, '1_point_field_goals': -1, '2_point_field_goals': -1, 'half_time': -1}\n",
    "    DONUT_DATA_2_WORDS = ['TRIES', 'CONVERSIONS', 'PENALTY GOALS',  'SIN BINS', '1 POINT FIELD GOALS','2 POINT FIELD GOALS', 'HALF TIME']\n",
    "\n",
    "    # Initialise competition level data\n",
    "    competition_team_data = []\n",
    "    competition_match_data = []\n",
    "    competition_try_data = []\n",
    "\n",
    "    for year in input_years:\n",
    "        print('Scraping data from the ' + str(year) + ' season')\n",
    "        \n",
    "        # Initialise year level data\n",
    "        year_team_data = []\n",
    "        year_match_data = []\n",
    "        year_try_data = []\n",
    "\n",
    "        for round in round_range:\n",
    "\n",
    "            # Initialise round level data\n",
    "            round_team_data = []\n",
    "            round_match_data = []\n",
    "            round_try_data = []\n",
    "            \n",
    "            for home, away in zip(home_team_input, away_team_input):\n",
    "                home, away = [x.replace(\" \", \"-\") for x in [home, away]]\n",
    "                \n",
    "                url = f\"{'https://www.nrl.com/draw/nrl-premiership/'}{year}/round-{round}/{home}-v-{away}/\"\n",
    "                print(f\"Scraping Round 1: {round} - {url}\")\n",
    "\n",
    "                # Webscrape the NRL WEBSITE\n",
    "                driver = set_up_driver()\n",
    "                driver.get(url)\n",
    "                page_source = driver.page_source\n",
    "                driver.quit()\n",
    "                soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "                home_possession, away_possession = None, None\n",
    "                try:\n",
    "                    # Home possession\n",
    "                    home_possession = soup.find(\n",
    "                        'p', class_='match-centre-card-donut__value--home').text.strip()\n",
    "                    away_possession = soup.find(\n",
    "                        'p', class_='match-centre-card-donut__value--away').text.strip()\n",
    "                except BaseException as BE:\n",
    "                    print(f\"Error in home possession {BE}\")\n",
    "\n",
    "                home_all_run_metres_list = soup.find_all(\n",
    "                    'dd',\n",
    "                    class_=[\n",
    "                        \"stats-bar-chart__label stats-bar-chart__label--home u-font-weight-700\",\n",
    "                        \"stats-bar-chart__label stats-bar-chart__label--home\"])\n",
    "                away_all_run_metres_list = soup.find_all(\n",
    "                    'dd',\n",
    "                    class_=[\n",
    "                        \"stats-bar-chart__label stats-bar-chart__label--away u-font-weight-700\",\n",
    "                        \"stats-bar-chart__label stats-bar-chart__label--away\"])\n",
    "\n",
    "                home_bars, away_bars = BARS_DATA.copy(), BARS_DATA.copy()\n",
    "\n",
    "                try:\n",
    "                    # Loop through each element\n",
    "                    for item, bar_name in zip(home_all_run_metres_list, home_bars.keys()):\n",
    "                        # Get the text of each element and strip any whitespace\n",
    "                        home_all_run_metres = item.get_text(strip=True)\n",
    "                        # Do whatever you want with the text\n",
    "                        home_bars[bar_name] = home_all_run_metres\n",
    "\n",
    "                    for item, bar_name in zip(away_all_run_metres_list, away_bars.keys()):\n",
    "                        # Get the text of each element and strip any whitespace\n",
    "                        home_all_run_metres = item.get_text(strip=True)\n",
    "                        # Do whatever you want with the text\n",
    "                        away_bars[bar_name] = home_all_run_metres\n",
    "                except BaseException:\n",
    "                    print(f\"Error with home bars\")\n",
    "\n",
    "                home_donut = DONUT_DATA.copy()\n",
    "                away_donut = DONUT_DATA.copy()\n",
    "                \n",
    "                try:\n",
    "                    elements = soup.find_all(\"p\", class_=\"donut-chart-stat__value\")\n",
    "                    # Loop through each element to extract the numbers\n",
    "                    numbers = []\n",
    "                    for element in elements:\n",
    "                        # Extract the text from the element\n",
    "                        text = element.get_text()\n",
    "                        # Find the number in the text\n",
    "                        number = ''.join(filter(lambda x: x.isdigit() or x == '.', text))\n",
    "                        numbers.append(number)\n",
    "                    home_donut.update({k: v for k, v in zip(home_donut, numbers[::2])})\n",
    "                    away_donut.update({k: v for k, v in zip(away_donut, numbers[1::2])})\n",
    "                except BaseException:\n",
    "                    print(\"error in donuts\")\n",
    "\n",
    "                # Initialise a list to store all names\n",
    "                home_try_names_list, home_try_minute_list = [], []\n",
    "\n",
    "                try:\n",
    "                    li_elements = soup.find(\n",
    "                        \"ul\", class_=\"match-centre-summary-group__list--home\").find_all(\"li\")\n",
    "\n",
    "                    # Loop through each <li> element and extract the name\n",
    "                    for li in li_elements:\n",
    "                        # Extract the text and remove leading/trailing whitespace\n",
    "                        text = li.get_text(strip=True)\n",
    "                        # Split the text at the space character\n",
    "                        parts = text.split()\n",
    "                        # Join the parts except the last one (which is the number) to get the\n",
    "                        # name\n",
    "                        name = ' '.join(parts[:-1])\n",
    "                        # Get the last part as the number\n",
    "                        number = parts[-1]\n",
    "                        # Append name and number to their respective lists\n",
    "                        home_try_names_list.append(name)\n",
    "                        home_try_minute_list.append(number)\n",
    "                except BaseException:\n",
    "                    print(\"error in home try scorers\")\n",
    "                home_first_try_scorer = home_try_names_list[0] if len(\n",
    "                    home_try_names_list) > 0 else None\n",
    "                home_first_minute_scorer = home_try_minute_list[0] if len(\n",
    "                    home_try_minute_list) > 0 else None\n",
    "\n",
    "                away_try_names_list = []\n",
    "                away_try_minute_list = []\n",
    "                try:\n",
    "                    li_elements = soup.find(\n",
    "                        \"ul\", class_=\"match-centre-summary-group__list--away\").find_all(\"li\")\n",
    "                    # Initialise a list to store all names\n",
    "\n",
    "                    # Loop through each <li> element and extract the name\n",
    "                    for li in li_elements:\n",
    "                        # Extract the text and remove leading/trailing whitespace\n",
    "                        text = li.get_text(strip=True)\n",
    "                        # Split the text at the space character\n",
    "                        parts = text.split()\n",
    "                        # Join the parts except the last one (which is the number) to get the\n",
    "                        # name\n",
    "                        name = ' '.join(parts[:-1])\n",
    "                        # Get the last part as the number\n",
    "                        number = parts[-1]\n",
    "                        # Append name and number to their respective lists\n",
    "                        away_try_names_list.append(name)\n",
    "                        away_try_minute_list.append(number)\n",
    "                except BaseException:\n",
    "                    print(\"error in away try scorers\")\n",
    "                away_first_try_scorer = away_try_names_list[0] if len(\n",
    "                    away_try_names_list) > 0 else None\n",
    "                away_first_minute_scorer = away_try_minute_list[0] if len(\n",
    "                    away_try_minute_list) > 0 else None\n",
    "\n",
    "                overall_first_try_scorer, overall_first_try_minute, overall_first_scorer_team = None, None, None\n",
    "                if away_first_try_scorer is None and home_first_try_scorer is None:\n",
    "                    overall_first_try_scorer = None\n",
    "                else:\n",
    "                    if away_first_minute_scorer is None:\n",
    "                        overall_first_try_scorer = home_first_try_scorer\n",
    "                        overall_first_try_minute = home_first_minute_scorer\n",
    "                        overall_first_scorer_team = home\n",
    "                    elif home_first_minute_scorer is None:\n",
    "                        overall_first_try_scorer = away_first_try_scorer\n",
    "                        overall_first_try_minute = away_first_minute_scorer\n",
    "                        overall_first_scorer_team = away\n",
    "                    elif away_first_minute_scorer > home_first_minute_scorer:\n",
    "                        overall_first_try_scorer = away_first_try_scorer\n",
    "                        overall_first_try_minute = away_first_minute_scorer\n",
    "                        overall_first_scorer_team = away\n",
    "                    else:\n",
    "                        overall_first_try_scorer = home_first_try_scorer\n",
    "                        overall_first_try_minute = home_first_minute_scorer\n",
    "                        overall_first_scorer_team = home\n",
    "\n",
    "                # Find all span elements with the specified class\n",
    "                span_elements = soup.find_all('span', class_='match-centre-summary-group__name')\n",
    "\n",
    "                # Check if any span element contains the desired text\n",
    "                for word in DONUT_DATA_2_WORDS:\n",
    "                    exists = any(span.text.strip().upper() == word for span in span_elements)\n",
    "                    if not exists:\n",
    "                        DONUT_DATA_2[word.lower().replace(' ', '_')] = -10\n",
    "                \n",
    "                home_game_stats, away_game_stats = DONUT_DATA_2.copy(), DONUT_DATA_2.copy()\n",
    "                \n",
    "                \n",
    "                numbers = []\n",
    "                try:\n",
    "                    span_elements = soup.find_all(\n",
    "                        \"span\", class_=\"match-centre-summary-group__value\")\n",
    "                    # Loop through each <span> element and extract the number\n",
    "                    for span_element in span_elements:\n",
    "                        numbers.append(span_element.span.get_text(strip=True))\n",
    "                        \n",
    "                    filtered_home_stats = {key: value for key, value in home_game_stats.items() if value != -10}\n",
    "\n",
    "                    for k, v in zip(filtered_home_stats, numbers[::2]):\n",
    "                        home_game_stats[k] = v\n",
    "\n",
    "                    for k, v in zip(filtered_home_stats, numbers[1::2]):\n",
    "                        away_game_stats[k] = v\n",
    "                        \n",
    "                except BaseException as Be:\n",
    "                    print(f\"Error with match top data {Be}\")\n",
    "                    \n",
    "\n",
    "                main_ref_name, ref_names, ref_positions = None, [], []\n",
    "                try:\n",
    "                    a_elements = soup.find_all(\"a\", class_=\"card-team-mate\")\n",
    "                    for a in a_elements:\n",
    "                        # Extract the name from <h3> element\n",
    "                        name = a.find(\"h3\",\n",
    "                                    class_=\"card-team-mate__name\").get_text(strip=True)\n",
    "                        ref_names.append(name)\n",
    "\n",
    "                        # Extract the position from <p> element\n",
    "                        position = a.find(\n",
    "                            \"p\", class_=\"card-team-mate__position\").get_text(strip=True)\n",
    "                        ref_positions.append(position)\n",
    "                    main_ref_name = ref_names[0]\n",
    "                except BaseException:\n",
    "                    print(\"error with ref data\")\n",
    "\n",
    "                # Initialise variables to store ground condition and weather condition\n",
    "                ground_condition, weather_condition = \"\", \"\"\n",
    "                try:\n",
    "                    # Find all <p> elements with class 'match-weather__text'\n",
    "                    p_elements = soup.find_all(\"p\", class_=\"match-weather__text\")\n",
    "\n",
    "                    # Loop through each <p> element and extract the text\n",
    "                    for p_element in p_elements:\n",
    "                        # Extract the text from the <span> element within the <p>\n",
    "                        condition_type = p_element.get_text(\n",
    "                            strip=True).split(\":\")[0].strip()\n",
    "                        condition_value = p_element.span.get_text(strip=True)\n",
    "\n",
    "                        # Check condition type and assign values accordingly\n",
    "                        if condition_type == \"Ground Conditions\":\n",
    "                            ground_condition = condition_value\n",
    "                        elif condition_type == \"Weather\":\n",
    "                            weather_condition = condition_value\n",
    "                except BaseException:\n",
    "                    print(\"error with conditions\")\n",
    "\n",
    "                # Join all the data togethor into an export\n",
    "                team_data_temp ={\n",
    "                    home+'.v.'+away: {\n",
    "                        home: {\n",
    "                                'home/away': 'home',\n",
    "                                'possession': home_possession,\n",
    "                                'first_try_scorer': home_first_try_scorer,\n",
    "                                'first_try_time': home_first_minute_scorer,\n",
    "                                **home_bars,\n",
    "                                **home_donut,\n",
    "                                **home_game_stats\n",
    "                            },\n",
    "                        away: {\n",
    "                                'home/away': 'away',\n",
    "                                'possession': away_possession,\n",
    "                                'first_try_scorer': away_first_try_scorer,\n",
    "                                'first_try_time': away_first_minute_scorer,\n",
    "                                **away_bars,\n",
    "                                **away_donut,\n",
    "                                **away_game_stats\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                \n",
    "                # Append the game data to the round data\n",
    "                round_team_data.append(team_data_temp)\n",
    "\n",
    "                match_data_temp = {\n",
    "                    home+'.v.'+away: {\n",
    "                        'overall_first_try_scorer': overall_first_try_scorer,\n",
    "                        'overall_first_try_minute': overall_first_try_minute,\n",
    "                        'overall_first_try_round': overall_first_scorer_team,\n",
    "                        #'ref_names': ref_names, #removed for now\n",
    "                        #'ref_positions': ref_positions,\n",
    "                        'main_ref': main_ref_name,\n",
    "                        'ground_condition': ground_condition,\n",
    "                        'weather_condition': weather_condition\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                # Append the game data to the round data\n",
    "                round_match_data.append(match_data_temp)\n",
    "\n",
    "                try_data_temp ={\n",
    "                    home+'.v.'+away: {\n",
    "                        home: {\n",
    "                                'home/away': 'home',\n",
    "                                'try_names': home_try_names_list, \n",
    "                                'try_minutes': home_try_minute_list, \n",
    "                            },\n",
    "                        away: {\n",
    "                                'home/away': 'away',\n",
    "                                'try_names': away_try_names_list,\n",
    "                                'try_minutes': away_try_minute_list,\n",
    "                            }\n",
    "                        }\n",
    "                }\n",
    "                                \n",
    "                # Append the game data to the round data\n",
    "                round_try_data.append(try_data_temp)\n",
    "\n",
    "            # Add round level hierarchy to json\n",
    "            round_team_data_combined = {\n",
    "                f\"{round}\": round_team_data\n",
    "            }\n",
    "\n",
    "            match_team_data_combined = {\n",
    "                f\"{round}\": round_match_data\n",
    "            }\n",
    "\n",
    "            match_try_data_combined = {\n",
    "                f\"{round}\": round_try_data\n",
    "            }\n",
    "\n",
    "        # Append round data to year data\n",
    "        year_team_data.append(round_team_data_combined)\n",
    "        year_match_data.append(match_team_data_combined)\n",
    "        year_try_data.append(match_try_data_combined)\n",
    "\n",
    "        # Add year level hierarchy to json\n",
    "        year_team_data_combined = {\n",
    "                    f\"{year}\": year_team_data\n",
    "                }\n",
    "        \n",
    "        year_match_data_combined = {\n",
    "                    f\"{year}\": year_match_data\n",
    "                }\n",
    "        \n",
    "        year_try_data_combined = {\n",
    "                    f\"{year}\": year_try_data\n",
    "                }\n",
    "\n",
    "    competition_team_data.append(year_team_data_combined)\n",
    "    competition_match_data.append(year_match_data_combined)\n",
    "    competition_try_data.append(year_try_data_combined)\n",
    "\n",
    "    master_team_data = {\n",
    "        \"NRL\": competition_team_data\n",
    "    }\n",
    "\n",
    "    master_match_data = {\n",
    "        \"NRL\": competition_match_data\n",
    "    }\n",
    "\n",
    "    master_try_data = {\n",
    "        \"NRL\": competition_try_data\n",
    "    }\n",
    "\n",
    "    # Write JSON data to a file\n",
    "    with open(f\"{output_destination}_match_statistics.json\", \"w\") as file:\n",
    "        file.write(json.dumps(master_team_data, indent=4))\n",
    "\n",
    "    with open(f\"{output_destination}_match_details.json\", \"w\") as file:\n",
    "        file.write(json.dumps(master_match_data, indent=4))\n",
    "\n",
    "    with open(f\"{output_destination}_match_try.json\", \"w\") as file:\n",
    "        file.write(json.dumps(master_try_data, indent=4))\n",
    "\n",
    "    print(f\"Tables has been written as {output_destination}\")\n",
    "\n",
    "    # Flatten the nested dictionary structure\n",
    "    \"\"\"\"\"\n",
    "    team_data_df = []\n",
    "    for competition, years in competition_team_data.items():\n",
    "        for year, rounds in competition.items():\n",
    "            for round_number, games in rounds.items():\n",
    "                for games, stats in game.items():\n",
    "                    row = {\n",
    "                        'Year': year,\n",
    "                        'Round': round_number,\n",
    "                        'Game': game,\n",
    "                    }\n",
    "                    row.update(stats)\n",
    "                    team_data_df.append(row)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(team_data_df)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    print(df)\n",
    "\n",
    "    # Write JSON data to a file\n",
    "    with open(f\"D:\\\\Downloads\\\\match_data_detailed_text.txt\", \"w\") as file:\n",
    "        file.write(df)\n",
    "\n",
    "    \"\"\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    home_team_input = ['sea eagles', 'roosters']\n",
    "    away_team_input = ['rabbitohs', 'broncos']\n",
    "    years = [2024]\n",
    "    rounds = [1]\n",
    "    output_destination = f\"D:\\\\Downloads\\\\2024_rd1\"\n",
    "    get_detailed_nrl_data(years, rounds, home_team_input, away_team_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
